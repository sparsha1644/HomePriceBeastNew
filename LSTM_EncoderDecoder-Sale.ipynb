{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84134ee8-6137-499f-9442-fc730d6cedf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import os, errno\n",
    "import sys\n",
    "from tqdm import trange\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "#https://github.com/lkulowski/LSTM_encoder_decoder/blob/master/code/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a535d28-7ad1-4ba3-b06c-bdbc4b2b188c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a08d94b-eeea-4335-9010-8c27fae6110c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = \"C:\\\\Users\\\\spars\\\\Documents\\\\Master\\\\JHU\\TML\\\\HomePriceBeastNew\\\\\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "07e18eae-02ad-4230-9778-0235e4780eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{data_folder}all_model_data_sale_model.npy\", 'rb') as f:\n",
    "    # Format of input x: (J, batch_size, num_features(k)), y: (H, batch_size, 1)\n",
    "    X_train = np.load(f, allow_pickle=True)\n",
    "    y_train = np.load(f, allow_pickle=True)\n",
    "    X_test = np.load(f, allow_pickle=True)\n",
    "    y_test = np.load(f, allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bc7a0930-cf0f-4c73-a124-c959662d7167",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{data_folder}all_model_labels_mapping_sale_model.npy\", 'rb') as f:\n",
    "    train_label = np.load(f, allow_pickle=True)\n",
    "    test_label = np.load(f, allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "37b61a63-e3f6-4fd4-bb87-b4301b937a39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 246249, 84)\n",
      "(4, 246249, 1)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a6b1d0f7-ca53-487a-9213-3a8b028bdde9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[220600. ],\n",
       "        [227850. ],\n",
       "        [239100. ],\n",
       "        ...,\n",
       "        [179112.5],\n",
       "        [188112.5],\n",
       "        [182312.5]],\n",
       "\n",
       "       [[227850. ],\n",
       "        [239100. ],\n",
       "        [254850. ],\n",
       "        ...,\n",
       "        [188112.5],\n",
       "        [182312.5],\n",
       "        [186062.5]],\n",
       "\n",
       "       [[239100. ],\n",
       "        [254850. ],\n",
       "        [266625. ],\n",
       "        ...,\n",
       "        [182312.5],\n",
       "        [186062.5],\n",
       "        [191362.5]],\n",
       "\n",
       "       [[254850. ],\n",
       "        [266625. ],\n",
       "        [253625. ],\n",
       "        ...,\n",
       "        [186062.5],\n",
       "        [191362.5],\n",
       "        [214737.5]]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.nan_to_num(X_train,copy=False)\n",
    "np.nan_to_num(y_train,copy=False)\n",
    "np.nan_to_num(X_test,copy=False)\n",
    "np.nan_to_num(y_test,copy=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4fb5db75-9703-4477-8888-3893cb9ff342",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_torch = torch.from_numpy(X_train).type(torch.Tensor).cuda()\n",
    "Y_train_torch = torch.from_numpy(y_train).type(torch.Tensor).cuda()\n",
    "\n",
    "X_test_torch = torch.from_numpy(X_test).type(torch.Tensor).cuda()\n",
    "Y_test_torch = torch.from_numpy(y_test).type(torch.Tensor).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "621e1635-0012-42ae-abac-9f49039cec31",
   "metadata": {},
   "outputs": [],
   "source": [
    "class inp_layer_norm(nn.Module):\n",
    "    def __init__(self,input_size):\n",
    "        '''\n",
    "        : param input_size:     the number of features in the input X\n",
    "        '''\n",
    "        \n",
    "        super(inp_layer_norm, self).__init__()\n",
    "        \n",
    "        self.layerNorm = nn.LayerNorm(input_size)\n",
    "        \n",
    "    def forward(self,x_input):\n",
    "        n_input = self.layerNorm(x_input)\n",
    "        \n",
    "        return n_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "322b101f-61e1-48dd-8fdb-0e7a0adcbd3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class lstm_final_output(nn.Module):\n",
    "    '''Takes the final decoder output and the output a single feature.'''\n",
    "    def __init__(self, input_size, output_size):\n",
    "        '''\n",
    "        : param input_size:     the number of features in the input X\n",
    "        : param output_size:    the number of features in the output Y\n",
    "        '''\n",
    "        \n",
    "        super(lstm_final_output, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size \n",
    "        \n",
    "        self.linear = nn.Linear(input_size, output_size)\n",
    "        \n",
    "    def forward(self, x_input):\n",
    "        \n",
    "        '''        \n",
    "        : param x_input:                    should be 2D (batch_size, input_size)\n",
    "        : param encoder_hidden_states:      hidden states\n",
    "        : return output, hidden:            output gives all the hidden states in the sequence;\n",
    "        :                                   hidden gives the hidden state and cell state for the last\n",
    "        :                                   element in the sequence \n",
    " \n",
    "        '''\n",
    "        output = self.linear(x_input.squeeze(0))     \n",
    "        \n",
    "        return output\n",
    "\n",
    "    \n",
    "class lstm_encoder(nn.Module):\n",
    "    ''' Encodes time-series sequence '''\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, dropout = 0.0, num_layers = 1):\n",
    "        \n",
    "        '''\n",
    "        : param input_size:     the number of features in the input X\n",
    "        : param hidden_size:    the number of features in the hidden state h\n",
    "        : param num_layers:     number of recurrent layers (i.e., 2 means there are\n",
    "        :                       2 stacked LSTMs)\n",
    "        '''\n",
    "        \n",
    "        super(lstm_encoder, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size \n",
    "        self.num_layers = num_layers\n",
    "       \n",
    "        # define LSTM layer\n",
    "        self.lstm = nn.LSTM(input_size = input_size, hidden_size = hidden_size, dropout=dropout,\n",
    "                            num_layers = num_layers)\n",
    "        \n",
    "        \n",
    "    def forward(self, x_input):\n",
    "        \n",
    "        '''\n",
    "        : param x_input:               input of shape (seq_len, # in batch, input_size)\n",
    "        : return lstm_out, hidden:     lstm_out gives all the hidden states in the sequence;\n",
    "        :                              hidden gives the hidden state and cell state for the last\n",
    "        :                              element in the sequence \n",
    "        '''\n",
    "\n",
    "        lstm_out, self.hidden = self.lstm(x_input.view(x_input.shape[0], x_input.shape[1], self.input_size))\n",
    "        \n",
    "        return lstm_out, self.hidden     \n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        \n",
    "        '''\n",
    "        initialize hidden state\n",
    "        : param batch_size:    x_input.shape[1]\n",
    "        : return:              zeroed hidden state and cell state \n",
    "        '''\n",
    "        \n",
    "        return (torch.zeros(self.num_layers, batch_size, self.hidden_size),\n",
    "                torch.zeros(self.num_layers, batch_size, self.hidden_size))\n",
    "\n",
    "\n",
    "class lstm_decoder(nn.Module):\n",
    "    ''' Decodes hidden state output by encoder '''\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, dropout=0.0, num_layers = 1):\n",
    "\n",
    "        '''\n",
    "        : param input_size:     the number of features in the input X\n",
    "        : param hidden_size:    the number of features in the hidden state h\n",
    "        : param num_layers:     number of recurrent layers (i.e., 2 means there are\n",
    "        :                       2 stacked LSTMs)\n",
    "        '''\n",
    "        \n",
    "        super(lstm_decoder, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size = input_size, hidden_size = hidden_size, dropout = dropout,\n",
    "                            num_layers = num_layers)\n",
    "        self.linear = nn.Linear(hidden_size, input_size)           \n",
    "\n",
    "    def forward(self, x_input, encoder_hidden_states):\n",
    "        \n",
    "        '''        \n",
    "        : param x_input:                    should be 2D (batch_size, input_size)\n",
    "        : param encoder_hidden_states:      hidden states\n",
    "        : return output, hidden:            output gives all the hidden states in the sequence;\n",
    "        :                                   hidden gives the hidden state and cell state for the last\n",
    "        :                                   element in the sequence \n",
    " \n",
    "        '''\n",
    "        lstm_out, self.hidden = self.lstm(x_input.unsqueeze(0), encoder_hidden_states)\n",
    "        output = self.linear(lstm_out.squeeze(0))     \n",
    "        \n",
    "        return output, self.hidden\n",
    "\n",
    "class lstm_seq2seq(nn.Module):\n",
    "    ''' train LSTM encoder-decoder and make predictions '''\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, output_size, dropout = 0.0, num_layers=1):\n",
    "\n",
    "        '''\n",
    "        : param input_size:     the number of expected features in the input X\n",
    "        : param hidden_size:    the number of features in the hidden state h\n",
    "        : param output_size:    the number of features in the output Y\n",
    "        '''\n",
    "\n",
    "        super(lstm_seq2seq, self).__init__()\n",
    "\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        self.layer_norm = inp_layer_norm(input_size = input_size)\n",
    "        self.encoder = lstm_encoder(input_size = input_size, hidden_size = hidden_size, dropout=dropout, num_layers=num_layers)\n",
    "        self.decoder = lstm_decoder(input_size = input_size, hidden_size = hidden_size, dropout=dropout, num_layers=num_layers)\n",
    "        self.outputter = lstm_final_output(input_size = input_size, output_size = output_size)\n",
    "\n",
    "    def train_model(self, input_tensor, target_tensor, n_epochs, target_len, batch_size, learning_rate = 0.01):\n",
    "        \n",
    "        '''\n",
    "        train lstm encoder-decoder\n",
    "        \n",
    "        : param input_tensor:              input data with shape (seq_len, # in batch, number features); PyTorch tensor    \n",
    "        : param target_tensor:             target data with shape (seq_len, # in batch, number features); PyTorch tensor\n",
    "        : param n_epochs:                  number of epochs \n",
    "        : param target_len:                number of values to predict \n",
    "        : param batch_size:                number of samples per gradient update\n",
    "        : param learning_rate:             float >= 0; learning rate\n",
    "        : return losses:                   array of loss function for each epoch\n",
    "        '''\n",
    "        \n",
    "        # initialize array of losses \n",
    "        losses = np.full(n_epochs, np.nan)\n",
    "        losses_per_horizon = np.full((n_epochs, target_len), np.nan)\n",
    "\n",
    "        optimizer = optim.Adam(self.parameters(), lr = learning_rate)\n",
    "        criterion = nn.MSELoss()\n",
    "\n",
    "        # calculate number of batch iterations\n",
    "        n_batches = int(input_tensor.shape[1] / batch_size)\n",
    "\n",
    "        with trange(n_epochs) as tr:\n",
    "            for it in tr:\n",
    "                \n",
    "                batch_loss = 0.\n",
    "                batch_loss_tf = 0.\n",
    "                batch_loss_no_tf = 0.\n",
    "                num_tf = 0\n",
    "                num_no_tf = 0\n",
    "\n",
    "                for b in range(n_batches):\n",
    "                    # select data \n",
    "                    input_batch = input_tensor[:, b: b + batch_size, :]\n",
    "                    target_batch = target_tensor[:, b: b + batch_size, :]\n",
    "\n",
    "                    # outputs tensor\n",
    "                    outputs = torch.zeros(target_len, batch_size, target_batch.shape[2]).cuda()\n",
    "\n",
    "                    # initialize hidden state\n",
    "                    encoder_hidden = self.encoder.init_hidden(batch_size)\n",
    "\n",
    "                    # zero the gradient\n",
    "                    optimizer.zero_grad()\n",
    "                    \n",
    "                    #Layer normalization\n",
    "                    input_batch = self.layer_norm(input_batch)\n",
    "\n",
    "                    # encoder outputs\n",
    "                    encoder_output, encoder_hidden = self.encoder(input_batch)\n",
    "\n",
    "                    # decoder with teacher forcing\n",
    "                    decoder_input = input_batch[-1, :, :]   # shape: (batch_size, input_size)\n",
    "                    decoder_hidden = encoder_hidden\n",
    "\n",
    "                    # predict recursively\n",
    "                    for t in range(target_len): \n",
    "                        decoder_output, decoder_hidden = self.decoder(decoder_input, decoder_hidden)\n",
    "                        outputs[t] = self.outputter(decoder_output)\n",
    "                        decoder_input = decoder_output\n",
    "\n",
    "                    # compute the loss \n",
    "                    loss = criterion(outputs, target_batch)\n",
    "                    loss_per_horizon = np.full(target_len, np.nan)\n",
    "                    for t in range(target_len): \n",
    "                        loss_per_horizon[t] = criterion(outputs[t], target_batch[t])\n",
    "                    batch_loss += loss.item()\n",
    "                    \n",
    "                    # backpropagation\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                # loss for epoch \n",
    "                batch_loss /= n_batches \n",
    "                losses[it] = batch_loss\n",
    "                for t in range(target_len):\n",
    "                    losses_per_horizon[it][t] = loss_per_horizon[t]\n",
    "                # progress bar \n",
    "                tr.set_postfix(loss=\"{0:.3f}\".format(batch_loss))\n",
    "                    \n",
    "        return losses, losses_per_horizon\n",
    "\n",
    "    def predict(self, input_tensor, target_len):\n",
    "        \n",
    "        '''\n",
    "        : param input_tensor:      input data (seq_len, input_size); PyTorch tensor \n",
    "        : param target_len:        number of target values to predict \n",
    "        : return np_outputs:       np.array containing predicted values; prediction done recursively \n",
    "        '''\n",
    "\n",
    "        # encode input_tensor\n",
    "        input_tensor = input_tensor.unsqueeze(1)     # add in batch size of 1\n",
    "        encoder_output, encoder_hidden = self.encoder(input_tensor)\n",
    "\n",
    "        # initialize tensor for predictions\n",
    "        outputs = torch.zeros(target_len, self.output_size).cuda()\n",
    "\n",
    "        # decode input_tensor\n",
    "        decoder_input = input_tensor[-1, :, :]\n",
    "        decoder_hidden = encoder_hidden\n",
    "        \n",
    "        for t in range(target_len):\n",
    "            decoder_output, decoder_hidden = self.decoder(decoder_input, decoder_hidden)\n",
    "            outputs[t] = self.outputter(decoder_output)\n",
    "            decoder_input = decoder_output\n",
    "            \n",
    "        np_outputs = outputs.cpu().detach().numpy()\n",
    "        \n",
    "        return np_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2b8737ab-3c1b-42df-aa9f-3c8a8ad106d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_train_test_results(lstm_model, Xtrain, Ytrain, Xtest, Ytest, num_rows = 4):\n",
    "    '''\n",
    "    plot examples of the lstm encoder-decoder evaluated on the training/test data\n",
    "\n",
    "    : param lstm_model: trained lstm encoder-decoder\n",
    "    : param Xtrain: np.array of windowed training input data\n",
    "    : param Ytrain: np.array of windowed training target data\n",
    "    : param Xtest: np.array of windowed test input data\n",
    "    : param Ytest: np.array of windowed test target data \n",
    "    : param num_rows: number of training/test examples to plot\n",
    "    : return: num_rows x 2 plots; first column is training data predictions,\n",
    "    : second column is test data predictions\n",
    "    '''\n",
    "    offset = random.randint(5, Ytest.shape[1]-num_rows)\n",
    "    # input window size\n",
    "    iw = Xtrain.shape[0]\n",
    "    ow = Ytest.shape[0]\n",
    "\n",
    "    # figure setup \n",
    "    num_cols = 2\n",
    "    num_plots = num_rows * num_cols\n",
    "\n",
    "    fig, ax = plt.subplots(num_rows, num_cols, figsize = (13, 15))\n",
    "\n",
    "    # plot training/test predictions\n",
    "    for ii in range(num_rows):\n",
    "        # train set\n",
    "        X_train_plt = Xtrain[:, offset+ii, :]\n",
    "        Y_train_pred = lstm_model.predict(torch.from_numpy(X_train_plt).type(torch.Tensor).cuda(), target_len = ow)\n",
    "\n",
    "        ax[ii, 0].plot(np.arange(0, iw), Xtrain[:, offset+ii, 0], 'k', linewidth = 2, label = 'Input')\n",
    "        ax[ii, 0].plot(np.arange(iw - 1, iw + ow), np.concatenate([[Xtrain[-1, offset+ii, 0]], Ytrain[:, offset+ii, 0]]),\n",
    "                                                                 color = (0.2, 0.42, 0.72), linewidth = 2, label = 'Target')\n",
    "        ax[ii, 0].plot(np.arange(iw - 1, iw + ow),        np.concatenate([[Xtrain[-1, offset+ii, 0]], Y_train_pred[:, 0]]),\n",
    "                                                                 color = (0.76, 0.01, 0.01), linewidth = 2, label = 'Prediction')\n",
    "        ax[ii, 0].set_xlim([0, iw + ow - 1])\n",
    "        ax[ii, 0].set_xlabel('$t$')\n",
    "        ax[ii, 0].set_ylabel('$y$')\n",
    "\n",
    "        # test set\n",
    "        X_test_plt = Xtest[:, offset+ii, :]\n",
    "        Y_test_pred = lstm_model.predict(torch.from_numpy(X_test_plt).type(torch.Tensor).cuda(), target_len = ow)\n",
    "        ax[ii, 1].plot(np.arange(0, iw), Xtest[:, offset+ii, 0], 'k', linewidth = 2, label = 'Input')\n",
    "        ax[ii, 1].plot(np.arange(iw - 1, iw + ow), np.concatenate([[Xtest[-1, offset+ii, 0]], Ytest[:, offset+ii, 0]]),\n",
    "                                                                 color = (0.2, 0.42, 0.72), linewidth = 2, label = 'Target')\n",
    "        ax[ii, 1].plot(np.arange(iw - 1, iw + ow), np.concatenate([[Xtest[-1, offset+ii, 0]], Y_test_pred[:, 0]]),\n",
    "                                                                 color = (0.76, 0.01, 0.01), linewidth = 2, label = 'Prediction')\n",
    "        ax[ii, 1].set_xlim([0, iw + ow - 1])\n",
    "        ax[ii, 1].set_xlabel('$t$')\n",
    "        ax[ii, 1].set_ylabel('$y$')\n",
    "\n",
    "        if ii == 0:\n",
    "                ax[ii, 0].set_title('Train')\n",
    "\n",
    "                ax[ii, 1].legend(bbox_to_anchor=(1, 1))\n",
    "                ax[ii, 1].set_title('Test')\n",
    "\n",
    "    plt.suptitle('LSTM Encoder-Decoder Predictions', x = 0.445, y = 1.)\n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(top = 0.95)\n",
    "    plt.show()\n",
    "#         plt.savefig('plots/predictions.png')\n",
    "    plt.close() \n",
    "\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "539db210-482b-4a7d-b5b2-c7b2ae99a6d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction_result(X_test, Ytest, test_label, lstm_model):\n",
    "    labels_county = test_label[0,:,0]\n",
    "    labels_period = test_label[1,:,0]\n",
    "    labels_state = test_label[2,:,0]\n",
    "    ow = Ytest.shape[0]\n",
    "    Y_test_pred = np.array([lstm_model.predict(torch.from_numpy(X_test[:,ii,:]).type(torch.Tensor).cuda(), target_len = ow) for ii in range(X_test.shape[1])])\n",
    "    df = pd.DataFrame()\n",
    "    df[\"county_name\"] = labels_county\n",
    "    df[\"period_begin\"] = labels_period\n",
    "    df[\"state_code\"] = labels_state\n",
    "    \n",
    "    for ix in range(ow):\n",
    "        df[f\"y_{ix+1}\"] = Ytest[ix,:,0]\n",
    "        df[f\"y_hat_{ix+1}\"] = Y_test_pred[:,ix,0]\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eed02d9a-8f53-4e30-9eca-64af493e78da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss_values(loss, losses_per_horizon):\n",
    "    n_epochs = len(loss)\n",
    "    target_len = losses_per_horizon.shape[1]\n",
    "    fig, ax = plt.subplots(2, 1, figsize = (8,8))\n",
    "    col_index = ['k', 'g', 'r', 'b', 'm']\n",
    "    for ii in range(target_len + 1):\n",
    "        if ii == 0:\n",
    "            ax[ii].plot(np.arange(1, n_epochs+1), loss, col_index[ii], linewidth = 2, label = 'Overall Loss')\n",
    "            ax[ii].legend(bbox_to_anchor=(1, 1))\n",
    "            ax[ii].set_title('Overall Loss')\n",
    "            ax[ii].set_xlabel('Epochs')\n",
    "            ax[ii].set_ylabel('Training Loss')\n",
    "        else:\n",
    "            ax[1].plot(np.arange(1, n_epochs+1), losses_per_horizon[:, ii-1], col_index[ii], linewidth = 1, label = f\"Horizon {ii} loss\")\n",
    "            ax[1].legend(bbox_to_anchor=(1, 1))\n",
    "            ax[1].set_title(f\"Horizon loss\")\n",
    "            ax[1].set_xlabel('Epochs')\n",
    "            ax[1].set_ylabel('Training Loss')\n",
    "    plt.suptitle('Loss curves', x = 0.445, y = 1.)\n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(top = 0.95)\n",
    "    plt.show()\n",
    "    #   plt.savefig('plots/predictions.png')\n",
    "    plt.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "34b5f9a8-5103-42f4-a28d-da7f855ffb38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = get_prediction_result(X_test, y_test, test_label, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2478a06a-2644-4f9f-8338-0d9025054e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def plot_mape_dist(df):\n",
    "#     for i in range(1,5):\n",
    "#         df[f\"mape_{i}\"] = math.abs(df[f\"y_hat_{i}\"] - df[f\"y_{i}\"])/(df[f\"y_{i}\"] + 0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5d4a7048-9847-4ff4-b704-9ca7244c773a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot predictions on train/test data\n",
    "# plot_train_test_results(model, \n",
    "#                          X_train, \n",
    "#                          y_train, \n",
    "#                          X_test, \n",
    "#                          y_test)\n",
    "\n",
    "# plt.close('all')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb39f27-3df0-4a38-bffc-347272f4c311",
   "metadata": {},
   "source": [
    "#Goals :\n",
    "3) Get MAPE plots. \n",
    "1) Make the model complex. \n",
    "4) Create a grid search for params and pick best model. \n",
    "5) Repeat this for median sale price without supply. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eefbb09-8962-455b-b75c-878a4e0e0553",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------\n",
      "HIDDEN SIZE 10 : NUM LAYERS : 2 : LEARNING RATE : 0.02 : DROP OUT : 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|███▌                                                        | 18/300 [00:25<07:00,  1.49s/it, loss=5062625321.796]"
     ]
    }
   ],
   "source": [
    "H = 3\n",
    "hidden_sizes = [10,15,30,60]\n",
    "layers = [2, 4]\n",
    "learning_rates = [0.02,0.05]\n",
    "dropouts = [0.0]\n",
    "n_epochs = 300\n",
    "batch_size = 5000\n",
    "\n",
    "best_model = None\n",
    "best_loss = float(\"inf\")\n",
    "best_losses_per_horizon = None\n",
    "\n",
    "for h in hidden_sizes:\n",
    "    for l in layers: \n",
    "        for lr in learning_rates:\n",
    "            for do in dropouts:\n",
    "                print(\"-------------------------------------\")\n",
    "                print(f\"HIDDEN SIZE {h} : NUM LAYERS : {l} : LEARNING RATE : {lr} : DROP OUT : {do}\")\n",
    "\n",
    "                model = lstm_seq2seq(input_size = X_train_torch.shape[2], hidden_size = h, dropout = do,\n",
    "                                     output_size = 1, num_layers=l).cuda()\n",
    "                loss, losses_per_horizon = model.train_model(X_train_torch, \n",
    "                                         Y_train_torch, \n",
    "                                         n_epochs = n_epochs, \n",
    "                                         target_len = (H+1), \n",
    "                                         batch_size = batch_size,\n",
    "                                         learning_rate = lr)\n",
    "                \n",
    "                if loss[n_epochs - 1] < best_loss:\n",
    "                    print(\"New best model found\")\n",
    "                    best_model = model\n",
    "                    best_loss = loss[n_epochs - 1]\n",
    "                    best_losses_per_horizon = losses_per_horizon\n",
    "                    \n",
    "                plot_loss_values(loss, losses_per_horizon)\n",
    "\n",
    "\n",
    "                # plot predictions on train/test data\n",
    "                plot_train_test_results(model, \n",
    "                                         X_train, \n",
    "                                         y_train, \n",
    "                                         X_test, \n",
    "                                         y_test)\n",
    "                \n",
    "                \n",
    "                print(\"-------------------------------------\")\n",
    "torch.save(best_model.state_dict(), './sale_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9966c6e4-8eb3-42d5-91aa-2286837fc5ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# H = 3\n",
    "# h = 30\n",
    "# l = 2\n",
    "# lr = 0.01\n",
    "# do = 0.0\n",
    "# n_epochs = 300\n",
    "# batch_size = 20000\n",
    "\n",
    "# model = lstm_seq2seq(input_size = X_train_torch.shape[2], hidden_size = h, dropout = do,\n",
    "#                      output_size = 1, num_layers=l).cuda()\n",
    "# loss, losses_per_horizon = model.train_model(X_train_torch, \n",
    "#                          Y_train_torch, \n",
    "#                          n_epochs = n_epochs, \n",
    "#                          target_len = (H+1), \n",
    "#                          batch_size = batch_size,\n",
    "#                          learning_rate = lr)\n",
    "# plot_loss_values(loss, losses_per_horizon)\n",
    "\n",
    "# torch.save(model.state_dict(), './current_sale_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "458a0166-50eb-4884-a003-254f3a5f2bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb40aa8e-60c9-4576-be37-ea74253ea15b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_train_test_results(model, X_train, y_train, X_test, y_test, num_rows = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91326424-cf21-488d-80b4-d61952a82294",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
